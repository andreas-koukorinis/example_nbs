{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named dev_analysis.market.get_data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-799090e8c563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msgmtradingcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mticks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatchbookTicks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBetfairTicks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBetdaqTicks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdev_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdev_analysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse_data\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named dev_analysis.market.get_data"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from os.path import expanduser\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import itertools\n",
    "from statsmodels.tsa import stattools\n",
    "\n",
    "from sgmtradingcore.exchange.ticks import MatchbookTicks, BetfairTicks, BetdaqTicks\n",
    "\n",
    "from dev_analysis.market.get_data import *\n",
    "from dev_analysis.market.analyse_data import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_list, fname_list = get_clean_df_list('/home/ioanna/.oddscache/', 30, has_date_folders = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_price_ticks = ['bp1','mipA', 'mipB', 'mdp', 'lp1']\n",
    "aux_prices_ticks = ['bp2', 'bp3', 'lp2', 'lp3']\n",
    "all_prices_ticks = main_price_ticks + aux_prices_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#get data for 200 stickers and clean up (only in-play, only matches with >10000 max matched volume, remove suspensions)\n",
    "\n",
    "def get_df_list(sticker_dir, sticker_no, inplay_only = True):\n",
    "    \n",
    "    stickers = glob.glob(os.path.join(sticker_dir, 'odds_T-EENP*'))\n",
    "    fname_list = stickers[0:sticker_no]\n",
    "    \n",
    "    df_list = []\n",
    "    \n",
    "    for fname in fname_list:\n",
    "        df = pd.read_csv(fname)\n",
    "        \n",
    "        if inplay_only == True:\n",
    "            df = df[df['delay']>0]\n",
    "         \n",
    "        #remove data with -1 prices\n",
    "        #gt = ((df['bp1'] != -1) & (df['bp2'] != -1) & (df['bp3'] != -1) & \\\n",
    "        #        (df['lp1'] != -1) & (df['lp2'] != -1) & (df['lp3'] != -1))\n",
    "        gt = (df['bp1'] != -1) & (df['lp1'] != -1)\n",
    "        df = df[gt]\n",
    "        \n",
    "        #only use data after certain amount has been matched\n",
    "        df = df[df['total'] > 1000]\n",
    "        \n",
    "        if (not df.empty) & (np.max(df['total']) > 10000):\n",
    "            df_list.append(df)\n",
    "            #dt = np.concatenate([np.zeros(1), np.diff(df['timestamp']/1000)])\n",
    "            #idx = np.where(dt ==  np.max(dt))[0][0]\n",
    "            #print df.iloc[idx-1:idx+2,:]\n",
    "        \n",
    "    return df_list, fname_list\n",
    "\n",
    "df_list, fname_list = get_df_list('../../media/ak/E3E1-EE52/.oddscache/', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function tocreate midprice, microprice and book pressure data\n",
    "\n",
    "def create_price_moves(df_list, fname_list, all_prices_ticks, wghts = [1, 0.3, 0.2]):\n",
    "\n",
    "    df_list_new = []\n",
    "\n",
    "    for df, fname in zip(df_list, fname_list):\n",
    "\n",
    "        #create df with timedelta index\n",
    "        df = df.reset_index()\n",
    "        tmdelta = pd.to_datetime(df['timestamp'].values, unit='ms')\n",
    "        df['timedelta'] = tmdelta\n",
    "        df.set_index(keys='timedelta', inplace = True)\n",
    "\n",
    "        #convert prices to ticks\n",
    "        if 'BF' in fname:\n",
    "            _ticks = BetfairTicks()\n",
    "        elif 'MB' in fname:\n",
    "            _ticks = MatchbookTicks()\n",
    "        elif 'BD' in fname:\n",
    "            _ticks = BetdaqTicks()\n",
    "        ps = ['bp1', 'bp2', 'bp3', 'lp1', 'lp2', 'lp3']\n",
    "        for p in ps:\n",
    "            df[p] = df[p].apply(lambda x: _ticks.convert_to(x))\n",
    "\n",
    "        #set -1 volumes to 0\n",
    "        vs = ['bv1', 'bv2', 'bv3', 'lv1', 'lv2', 'lv3']\n",
    "        for v in vs:\n",
    "            df[v][df[v]<0] = 0\n",
    "\n",
    "\n",
    "        #create spread\n",
    "        df['sp_ticks'] = df['lp1'] - df['bp1']\n",
    "        df = df[df['sp_ticks'] < 50]\n",
    "\n",
    "\n",
    "        #create midprice column\n",
    "        df['mdp'] = (df['bp1'] + df['lp1'])/2\n",
    "\n",
    "\n",
    "        #create book pressure (need to add weights)\n",
    "        bv = wghts[0]*df['bv1'] + wghts[1]*df['bv2'] + wghts[2]*df['bv3']\n",
    "        lv = wghts[0]*df['lv1'] + wghts[1]*df['lv2'] + wghts[2]*df['lv3']\n",
    "        df['bp'] = (bv - lv)/(bv + lv)\n",
    "        \n",
    "        #for median\n",
    "        df['bp_med'] = (bv - lv)/(bv + lv)\n",
    "        \n",
    "        #bp change\n",
    "        df['bp_move'] = df['bp'].diff().fillna(0)\n",
    "\n",
    "        #create microprice column\n",
    "        df['mipB'] = (df['bp1']*df['lv1'] + df['lp1']*df['bv1'])/(df['lv1'] + df['bv1'])\n",
    "        df['mipA'] = (df['bp1']*df['bv1'] + df['lp1']*df['lv1'])/(df['lv1'] + df['bv1'])\n",
    "\n",
    "        for price_ticks in all_prices_ticks:\n",
    "            #price_ticks = 'LP3_ticks'\n",
    "            price_ticks_move = price_ticks + '_move'\n",
    "\n",
    "            # add microprice move in ticks\n",
    "            df[price_ticks_move] = df[price_ticks].diff().fillna(0)\n",
    "            #df['price_ticks_move_name'] = price_ticks_move\n",
    "\n",
    "        df_list_new.append(df)\n",
    "        \n",
    "    return df_list_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to resample data to given time interval\n",
    "\n",
    "def resample_df(df_list_new, time_sample = '5S'):\n",
    "  \n",
    "    op_price = np.sum\n",
    "    price_cols = [price_tick + '_move' for price_tick in all_prices_ticks]#[price_ticks_move]\n",
    "    how_resample = {col: op_price for col in price_cols}\n",
    "    \n",
    "    op_price = np.mean\n",
    "    price_cols = [price_tick for price_tick in all_prices_ticks]#[price_ticks_move]\n",
    "    how_resample.update({col: op_price for col in price_cols})\n",
    "\n",
    "    op_signal = np.median\n",
    "    signal_cols = ['bp_med']\n",
    "    how_resample.update({col: op_signal for col in signal_cols})\n",
    "    \n",
    "    op_signal = np.mean\n",
    "    tick_cols = ['sp_ticks', 'bp_move', 'bp']\n",
    "    how_resample.update({col: op_signal for col in tick_cols})\n",
    "\n",
    "    #how_resample.update({'price_ticks_move_name' : 'first'})\n",
    "\n",
    "    # resample each df\n",
    "    df_list_resample = []\n",
    "    for df in df_list_new:\n",
    "        df_list_resample.append(df.resample(time_sample).agg(how_resample))\n",
    "\n",
    "    data_resample = pd.concat(df_list_resample, axis=0)\n",
    "    \n",
    "    #remove nans\n",
    "    data_resample = data_resample[pd.notnull(data_resample['bp'])]\n",
    "    \n",
    "    return data_resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We basically have 3 parameters to play around with:\n",
    "\n",
    "a) **weights** used to calculate book pressure. We assume that level 1 imbalance is likely to be a better indicator than level 2, etc, and we explore a combination of weights that satisfies w1>w2>w3.\n",
    "\n",
    "b) **time step** to resample dataframe to. We explore values between 1s and 20s.\n",
    "\n",
    "c) **book pressure shift**. After we resample our dataframe, we end up with time intervals of x seconds and the corresponding book pressure and change in midprice(/microprice) values for each interval compared to the last one. We want to see how well bp predicts price shift in subsequent time intervals, so we shift the bp column by n rows down before doing regression. we try n=0,1,2,3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All objects passed were None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-180c4a29e81d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_list_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_price_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_prices_ticks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwghts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata_resample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-173419a6dd6e>\u001b[0m in \u001b[0;36mresample_df\u001b[0;34m(df_list_new, time_sample)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdf_list_resample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow_resample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdata_resample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list_resample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#remove nans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/researchteam/anaconda/envs/rt_ipynb/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    752\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    755\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/researchteam/anaconda/envs/rt_ipynb/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'All objects passed were None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;31m# consolidate data & figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All objects passed were None"
     ]
    }
   ],
   "source": [
    "#sample plotting for a choice of BP weights, time interval, and BP shift\n",
    "\n",
    "time_sample = '2S'\n",
    "\n",
    "df_list_new = create_price_moves(df_list, fname_list, all_prices_ticks, wghts = [1, 0.3, 0.2])\n",
    "data_resample = resample_df(df_list_new, time_sample = time_sample)\n",
    "\n",
    "\n",
    "#plot stuff\n",
    "\n",
    "x = 'bp'\n",
    "bp_bins = np.arange(-1, 1, 0.15)\n",
    "colors = ['b', 'r', 'g', 'm']\n",
    "\n",
    "iis = range(0,4)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "for i in iis:\n",
    "    sns.regplot(data_resample[x].shift(i), data_resample['mdp_move'], order=3, x_bins = bp_bins, \n",
    "          scatter=True, truncate=True, ci=66, n_boot=100, color=colors[i], label='bp ' + str(i) + ' timesteps before')\n",
    "ax.legend()\n",
    "sns.plt.title('Price movements within time interval of %s'%time_sample)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "for i in range(0,4):\n",
    "    sns.regplot(data_resample[x].shift(i), data_resample['mipA_move'], order=3, x_bins = bp_bins, \n",
    "          scatter=True, truncate=True, ci=66, n_boot=100, color=colors[i], label='bp ' + str(i) + ' timesteps before')\n",
    "ax.legend()\n",
    "\n",
    "    \n",
    "f, ax = plt.subplots(figsize=(8,5))\n",
    "for i in range(0,4):\n",
    "    sns.regplot(data_resample[x].shift(i), data_resample['mipB_move'], order=3, x_bins = bp_bins, \n",
    "          scatter=True, truncate=True, ci=66, n_boot=100, color=colors[i], label='bp ' + str(i) + ' timesteps before')\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#granger causality\n",
    "\n",
    "stattools.grangercausalitytests(data_resample[['bp', 'mdp']], maxlag = 2);\n",
    "stattools.grangercausalitytests(data_resample[['bp', 'mipA']], maxlag = 2);\n",
    "stattools.grangercausalitytests(data_resample[['bp', 'mipB']], maxlag = 2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit both linear and logistic regressions for various BP weights and deltas for midprice\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def get_regression_matrix(x_name, y_name, aas, ts, shifts, df_list, fname_list, all_prices_ticks):\n",
    "\n",
    "    scores = pd.DataFrame()\n",
    "    r2s = pd.DataFrame()\n",
    "    lgfits = pd.DataFrame()\n",
    "    lrfits = pd.DataFrame()\n",
    "\n",
    "    tns = [str(t)+'S' for t in ts]\n",
    "\n",
    "    for a in aas:\n",
    "\n",
    "        wghts = [1, round(a,3), round(a**1.5,3)]\n",
    "\n",
    "        df_list_new = create_price_moves(df_list, fname_list, all_prices_ticks, wghts = wghts)\n",
    "\n",
    "        for tn in tns:\n",
    "            \n",
    "            data_resample = resample_df(df_list_new, time_sample = tn)\n",
    "\n",
    "            for shift in shifts:\n",
    "                \n",
    "                print 'weights = ' + str(wghts) + ', time interval = ' + str(tn) + ', shift = ' + str(shift)\n",
    "                \n",
    "                #assign data for fitting\n",
    "                BP = data_resample[x_name].shift(shift)\n",
    "                dm = data_resample[y_name]\n",
    "                \n",
    "                #deal with nans\n",
    "                gt = pd.notnull(BP)\n",
    "                BP = BP[gt]\n",
    "                dm = dm[gt]\n",
    "                \n",
    "                #only model sensible price shifts (<10 ticks)\n",
    "                tgt = abs(dm) < 10\n",
    "                BP = BP.loc[tgt]\n",
    "                dm = dm[tgt]\n",
    "\n",
    "                #only model considerable price shifts (>2 ticks)\n",
    "                tgt = abs(dm) > 2\n",
    "                BP = BP.loc[tgt]\n",
    "                dm = dm[tgt]\n",
    "                \n",
    "                #split into train/test (seed to get same result every time)\n",
    "                BP_train, BP_test, dm_train, dm_test = \\\n",
    "                    train_test_split(BP, dm, test_size = 0.2, random_state = 1)\n",
    "\n",
    "                #logistic regression\n",
    "                lg = LogisticRegression(class_weight = 'balanced')\n",
    "                lgfit = lg.fit(BP_train.values.reshape(-1, 1), dm_train>0)\n",
    "                scores.loc[str(wghts), 'sh' + str(shift) + ', ' + tn] = lgfit.score(BP_test.values.reshape(-1, 1), dm_test>0)\n",
    "                lgfits.loc[str(wghts), 'sh' + str(shift) + ', ' + tn] = lgfit\n",
    "                \n",
    "                #linear regression\n",
    "                lr = LinearRegression()\n",
    "                lrfit = lr.fit(BP_train.values.reshape(-1, 1), dm_train)\n",
    "                r2s.loc[str(wghts), 'sh' + str(shift) + ', ' + tn] = lrfit.score(BP_test.values.reshape(-1, 1), dm_test)\n",
    "                lrfits.loc[str(wghts), 'sh' + str(shift) + ', ' + tn] = lrfit\n",
    "                \n",
    "    return scores, r2s, lgfits, lrfits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now calculate scores and R2 for logistic and linear regression respectively for a matrix of values for our 3 parameters, for midprice, micropriceA and micropriceB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aas = np.arange(0, 0.6, 0.1)\n",
    "ts = [1,2,5,10,15]\n",
    "shifts = range(0,3)\n",
    "\n",
    "#mean bp\n",
    "scores_bp_mipA, r2s_bp_mipA, lgfits_bp_mipA, lrfits_bp_mipA = \\\n",
    "    get_regression_matrix('bp', 'mipA_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)\n",
    "    \n",
    "scores_bp_mipB, r2s_bp_mipB, lgfits_bp_mipB, lrfits_bp_mipB = \\\n",
    "    get_regression_matrix('bp', 'mipB_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)\n",
    "    \n",
    "scores_bp_mdp, r2s_bp_mdp, lgfits_bp_mdp, lrfits_bp_mdp = \\\n",
    "    get_regression_matrix('bp', 'mdp_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delta bp\n",
    "scores_bpmove_mipA, r2s_bpmove_mipA, lgfits_bpmove_mipA, lrfits_bpmove_mipA = \\\n",
    "    get_regression_matrix('bp_move', 'mipA_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)\n",
    "    \n",
    "scores_bpmove_mipB, r2s_bpmove_mipB, lgfits_bpmove_mipB, lrfits_bpmove_mipB = \\\n",
    "    get_regression_matrix('bp_move', 'mipB_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)\n",
    "    \n",
    "scores_bpmove_mdp, r2s_bpmove_mdp, lgfits_bpmove_mdp, lrfits_bpmove_mdp = \\\n",
    "    get_regression_matrix('bp_move', 'mdp_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median bp\n",
    "scores_bpmed_mipA, r2s_bpmed_mipA, lgfits_bpmed_mipA, lrfits_bpmed_mipA = \\\n",
    "    get_regression_matrix('bp_med', 'mipA_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)\n",
    "    \n",
    "scores_bpmed_mipB, r2s_bpmed_mipB, lgfits_bpmed_mipB, lrfits_bpmed_mipB = \\\n",
    "    get_regression_matrix('bp_med', 'mipB_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)\n",
    "    \n",
    "scores_bpmed_mdp, r2s_bpmed_mdp, lgfits_bpmed_mdp, lrfits_bpmed_mdp = \\\n",
    "    get_regression_matrix('bp_med', 'mdp_move', aas, ts, shifts, df_list, fname_list, all_prices_ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot heatmap of logistic regression scores\n",
    "\n",
    "def plot_heatmaps(scores, r2s):\n",
    "    \n",
    "    f, ax = plt.subplots(nrows = 3, ncols = 2, figsize=(15,17))\n",
    "    #f.tight_layout()\n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.44, hspace=0.6)\n",
    "\n",
    "    ########################### scores ###################################\n",
    "    gt = [col for col in scores.columns if ('sh1' in col) or ('sh2' in col)]\n",
    "    sns.heatmap(scores.loc[:,gt], ax=ax[0][0])#, cmap = 'plasma', vmin=vmn, vmax=vmx)\n",
    "    vmn = ax[0][0].collections[0].colorbar.vmin\n",
    "    vmx = ax[0][0].collections[0].colorbar.vmax\n",
    "    \n",
    "    gt = [col for col in scores.columns if 'sh1' in col]\n",
    "    sns.heatmap(scores.loc[:,gt], ax=ax[1][0], vmin=vmn, vmax=vmx)\n",
    "    \n",
    "    gt = [col for col in scores.columns if 'sh2' in col]\n",
    "    sns.heatmap(scores.loc[:,gt], ax=ax[2][0], vmin=vmn, vmax=vmx)\n",
    "\n",
    "    \n",
    "    ############################ r2s ###################################\n",
    "    gt = [col for col in r2s.columns if ('sh1' in col) or ('sh2' in col)]\n",
    "    sns.heatmap(r2s.loc[:,gt], ax=ax[0][1])\n",
    "    vmn = ax[0][1].collections[0].colorbar.vmin\n",
    "    vmx = ax[0][1].collections[0].colorbar.vmax\n",
    "    \n",
    "    gt = [col for col in r2s.columns if 'sh1' in col]\n",
    "    sns.heatmap(r2s.loc[:,gt], ax=ax[1][1], vmin=vmn, vmax=vmx)\n",
    "    \n",
    "    gt = [col for col in scores.columns if 'sh2' in col]\n",
    "    sns.heatmap(r2s.loc[:,gt], ax=ax[2][1], vmin=vmn, vmax=vmx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Midprice\n",
    "\n",
    "We see that constructing book pressure using only first level volumes (i.e. weights of [1,0,0]) produces the best results. \n",
    "\n",
    "In general, the shift1 predictions (i.e. using book pressure at current time interval to predict midprice movement one interval ahead) are more accurate than the shift2 (predicting two intervals ahead). This is especially clear when the size of the time intervals increases from 1s to 2s. \n",
    "\n",
    "When we use the change in book pressure to predict price movements, the smallest the time interval the best the accuracy. Note that the delta in book pressure is a much better predictor than the absolute bp (logistic regression scores ~0.66 compared to ~0.54).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#midprice\n",
    "plot_heatmaps(scores_bp_mdp, r2s_bp_mdp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#using delta book pressure as the predictor\n",
    "plot_heatmaps(scores_bpmove_mdp, r2s_bpmove_mdp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion, for midprice prediction we will use a 1sec time interval, and combine the bp_move from the last 3 intervals as well as the absolute bp value from the last 2 intervals as a starting point for our fit.\n",
    "\n",
    "As a secondary fit, we will use a 2sec interval with the last 2 bp_move values and the last bp value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Microprice A\n",
    "\n",
    "$mipA = (bp1\\cdot bv1 + lp1\\cdot lv1)/(lv1 + bv1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#micropriceA\n",
    "plot_heatmaps(scores_bp_mipA, r2s_bp_mipA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps(scores_bpmove_mipA, r2s_bpmove_mipA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Microprice B\n",
    "\n",
    "$mipB = (bp1\\cdot lv1 + lp1\\cdot bv1)/(lv1 + bv1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#micropriceB\n",
    "plot_heatmaps(scores_bp_mipB, r2s_bp_mipB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmaps(scores_bpmove_mipB, r2s_bpmove_mipB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Midprice fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) time interval = 1sec, weightings = [1,0,0], bp_move from 3 previous timesteps and bp from 2 previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list_new = create_price_moves(df_list, fname_list, all_prices_ticks, wghts = [1,0,0])            \n",
    "data_resample = resample_df(df_list_new, time_sample = '1S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,a = plt.subplots(figsize=(10,7))\n",
    "gt = abs(data_resample['bp_move']) > 0.1\n",
    "plt.hist(data_resample.loc[gt,'bp_move'], bins=10);\n",
    "#a.set_xlim([-10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assign x data\n",
    "all_x = [data_resample['bp_move'].shift(1), data_resample['bp_move'].shift(2), data_resample['bp_move'].shift(3),\\\n",
    "          data_resample['bp'].shift(1), data_resample['bp'].shift(2)]\n",
    "#all_x = [data_resample['bp_move'].shift(1), data_resample['bp'].shift(1)]\n",
    "#all_x = [data_resample['bp_move'].shift(1)]\n",
    "BP = pd.concat(all_x, axis = 1)\n",
    "\n",
    "#assign y\n",
    "dm = data_resample.loc[:,'mdp_move']\n",
    "\n",
    "#deal with nans\n",
    "gt = pd.notnull(BP)\n",
    "BP = BP[gt.all(axis = 1)]\n",
    "dm = dm[gt.all(axis = 1)]\n",
    "\n",
    "#only model sensible price shifts (<10 ticks)\n",
    "tgt = abs(dm) < 100\n",
    "BP = BP.loc[tgt,:]\n",
    "dm = dm[tgt]\n",
    "\n",
    "#only model considerable price shifts (>2 ticks) \n",
    "#TODO: what does this mean when we use the classifier in practice though? \n",
    "#we will end up always predicting a price change, which is not brilliant\n",
    "#if we include 0 move though we cannot balance the classes\n",
    "#we could use something else to assess whether a change is likely to happen and combine?\n",
    "tgt = abs(dm) > 0.01\n",
    "BP = BP.loc[tgt,:]\n",
    "dm = dm[tgt]\n",
    "\n",
    "#remove bp_move too small\n",
    "#gt = abs(BP.iloc[:,0]) > 0.1\n",
    "#BP = BP.loc[gt,:]\n",
    "#dm = dm[gt]\n",
    "\n",
    "clas = [-100,-3, -.1, .1, 3, 100]\n",
    "clas = [-100, -0.001, 0.001, 100]\n",
    "\n",
    "clas_lab = range(-len(clas)/2+1, len(clas)/2)\n",
    "dm_cat = pd.cut(dm, clas, labels = clas_lab)\n",
    "\n",
    "#deal with nans\n",
    "gt = pd.notnull(dm_cat)\n",
    "BP = BP.loc[gt,:]\n",
    "dm_cat = dm_cat[gt]\n",
    "dm = dm[gt]\n",
    "\n",
    "#split into train/test (seed to get same result every time)\n",
    "BP_train, BP_test, dm_cat_train, dm_cat_test, dm_train, dm_test = \\\n",
    "    train_test_split(BP, dm_cat, dm, test_size = 0.2, random_state = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     41
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_selection import f_regression, f_classif\n",
    "\n",
    "#chi2, pval = f_regression(BP_train, dm_train)\n",
    "\n",
    "#print chi2\n",
    "#print pval\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def show_confusion_matrix(y_test, y_pred, class_names):    \n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(cnf_matrix, classes = class_names,\n",
    "    #                      title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(cnf_matrix, classes = class_names, normalize=True,\n",
    "    #                      title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "#xr = np.arange(-2,2,0.01)\n",
    "#yr = np.arange(-1,1,0.01)\n",
    "\n",
    "#logistic regression\n",
    "lg = LogisticRegression(class_weight = 'balanced')\n",
    "#lg = LogisticRegression(class_weight = dict(zip(np.unique(dm_cat_train), [1,0.06,1])))\n",
    "\n",
    "print 'logistic, balanced, pred: bp_move'\n",
    "x_train = BP_train.iloc[:,0].values.reshape(-1, 1)\n",
    "x_test  = BP_test.iloc[:,0].values.reshape(-1, 1)\n",
    "lgfit = lg.fit(x_train, dm_cat_train)\n",
    "print classification_report(dm_cat_test, lgfit.predict(x_test)) \n",
    "show_confusion_matrix(dm_cat_test, lgfit.predict(x_test), np.unique(dm_cat_train)) \n",
    "#plt.plot(xr, (-lgfit.intercept_/lgfit.coef_[0])*np.ones([len(xr),1]))\n",
    "\n",
    "print 'logistic, balanced, pred: bp_move>0'\n",
    "x_train = BP_train.iloc[:,0].values.reshape(-1, 1)>0\n",
    "x_test  = BP_test.iloc[:,0].values.reshape(-1, 1)>0\n",
    "lgfit = lg.fit(x_train, dm_cat_train)\n",
    "print classification_report(dm_cat_test, lgfit.predict(x_test))   \n",
    "show_confusion_matrix(dm_cat_test, lgfit.predict(x_test), np.unique(dm_cat_train)) \n",
    "\n",
    "\n",
    "print 'logistic, balanced, pred: bp'\n",
    "x_train = BP_train.iloc[:,3].values.reshape(-1, 1)\n",
    "x_test = BP_test.iloc[:,3].values.reshape(-1, 1)\n",
    "lgfit = lg.fit(x_train, dm_cat_train)\n",
    "print classification_report(dm_cat_test, lgfit.predict(x_test))\n",
    "show_confusion_matrix(dm_cat_test, lgfit.predict(x_test), np.unique(dm_cat_train)) \n",
    "\n",
    "\n",
    "#plt.plot((-lgfit.intercept_/lgfit.coef_[0] )*np.ones([len(yr),1]), yr, 'r')\n",
    "\n",
    "print 'logistic, balanced, pred: bp_move, bp'\n",
    "x_train = BP_train.iloc[:,[0,3]]\n",
    "x_test  = BP_test.iloc[:,[0,3]]\n",
    "lgfit = lg.fit(x_train, dm_cat_train)\n",
    "print classification_report(dm_cat_test, lgfit.predict(x_test)) \n",
    "show_confusion_matrix(dm_cat_test, lgfit.predict(x_test), np.unique(dm_cat_train)) \n",
    "\n",
    "\n",
    "print 'logistic, balanced, pred: bp_move>0, bp'\n",
    "x_train = pd.concat([BP_train.iloc[:,0]>0, BP_train.iloc[:,3]], axis=1)\n",
    "x_test  = pd.concat([BP_test.iloc[:,0]>0, BP_test.iloc[:,3]], axis=1)\n",
    "lgfit = lg.fit(x_train, dm_cat_train)\n",
    "print classification_report(dm_cat_test, lgfit.predict(x_test))  \n",
    "show_confusion_matrix(dm_cat_test, lgfit.predict(x_test), np.unique(dm_cat_train)) \n",
    "\n",
    "\n",
    "#plt.plot(xr, (-lgfit.intercept_[0] - lgfit.coef_[0][0]*xr)/lgfit.coef_[0][1], 'g')\n",
    "\n",
    "print 'SVC, balanced, pred: bp_move, bp'\n",
    "lsvc = LinearSVC(class_weight = 'balanced')\n",
    "x_train = BP_train.iloc[:,[0,3]]\n",
    "x_test  = BP_test.iloc[:,[0,3]]\n",
    "lsvcfit = lsvc.fit(x_train, dm_cat_train)\n",
    "print classification_report(dm_cat_test, lsvcfit.predict(x_test))  \n",
    "show_confusion_matrix(dm_cat_test, lsvcfit.predict(x_test), np.unique(dm_cat_train)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, everything gives us similar answers (when similarly balanced), and all methods struggle to distinguish within different negative and positive classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(12,10))\n",
    "cols = ['b', 'r', 'g', 'y', 'm']\n",
    "labels = ['bp_move_sh1', 'bp_move_sh2', 'bp_move_sh3', 'bp_sh1', 'bp_sh2']\n",
    "for i in range(0,3):\n",
    "\n",
    "    sns.regplot(BP.iloc[:,i], dm, x_bins = 20, color = cols[i], ax = ax, label = labels[i], order=3)\n",
    "ax.legend()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(12,10))\n",
    "cols = ['b', 'r', 'g', 'y', 'm']\n",
    "labels = ['bp_move_sh1', 'bp_move_sh2', 'bp_move_sh3', 'bp_sh1', 'bp_sh2']\n",
    "for i in range(3,5):\n",
    "\n",
    "    sns.regplot(BP.iloc[:,i], dm, x_bins = 20, color = cols[i], ax = ax, label = labels[i], order=3)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pd.concat([BP.iloc[:,[0,3]], pd.cut(dm,2)], axis = 1)\n",
    "\n",
    "sns.lmplot(x = 'bp_move', y = 'bp', hue = 'mdp_move', data = xy.sample(frac=0.1), scatter = True, size=7, aspect = 2)\n",
    "\n",
    "\n",
    "sns.lmplot(x = 'bp_move', y = 'bp', hue = 'mdp_move', x_bins = 40, data = xy.sample(frac=1), size=7, aspect = 2)\n",
    "\n",
    "sns.lmplot(x = 'bp', y = 'bp_move', hue = 'mdp_move', data = xy.sample(frac=0.1), scatter = True, size=7, aspect = 2)\n",
    "\n",
    "\n",
    "sns.lmplot(x = 'bp', y = 'bp_move', hue = 'mdp_move', x_bins = 40, data = xy.sample(frac=1), size=7, aspect = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = pd.concat([BP.iloc[:,[0,3]], pd.cut(dm,6)], axis = 1)\n",
    "\n",
    "sns.lmplot(x = 'bp_move', y = 'bp', hue = 'mdp_move', data = xy.sample(frac=0.1), scatter = True, size=7, aspect = 2)\n",
    "\n",
    "\n",
    "sns.lmplot(x = 'bp_move', y = 'bp', hue = 'mdp_move', x_bins = 40, data = xy.sample(frac=1), size=7, aspect = 2)\n",
    "\n",
    "sns.lmplot(x = 'bp', y = 'bp_move', hue = 'mdp_move', data = xy.sample(frac=0.1), scatter = True, size=7, aspect = 2)\n",
    "\n",
    "\n",
    "sns.lmplot(x = 'bp', y = 'bp_move', hue = 'mdp_move', x_bins = 40, data = xy.sample(frac=1), size=7, aspect = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = (abs(BP.iloc[:,0]) > -0.001) \n",
    "\n",
    "\n",
    "x1 = BP.iloc[:,0]\n",
    "x = pd.concat([x1, BP.iloc[:,3]], axis=1)\n",
    "\n",
    "\n",
    "y = pd.cut(dm.loc[gt],6)\n",
    "xy = pd.concat([x.loc[gt,:], y], axis = 1)\n",
    "\n",
    "sns.jointplot(x='bp_move', y='bp', data=xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
